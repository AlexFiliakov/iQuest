name: Performance Benchmarks

on:
  pull_request:
    paths:
      - 'src/**'
      - 'tests/**'
      - 'requirements*.txt'
      - '.github/workflows/benchmarks.yml'
  schedule:
    - cron: '0 0 * * *'  # Daily at midnight UTC
  workflow_dispatch:  # Allow manual triggers

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.9', '3.10', '3.11']
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0  # Need full history for comparisons
      
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      
      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.cache/pip
            ~/.cache/pytest
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest-json-report
      
      - name: Run performance benchmarks
        run: |
          pytest tests/performance/ tests/test_performance_benchmarks.py \
            -m "performance or benchmark" \
            --benchmark-only \
            --benchmark-json=benchmark_results.json \
            --benchmark-autosave \
            --benchmark-max-time=1.0 \
            --benchmark-min-rounds=3 \
            --json-report --json-report-file=test_results.json
        continue-on-error: true
      
      - name: Upload benchmark results
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results-${{ matrix.os }}-py${{ matrix.python-version }}
          path: |
            benchmark_results.json
            test_results.json
            .benchmarks/
      
      - name: Store benchmark result for comparison
        uses: benchmark-action/github-action-benchmark@v1
        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
        with:
          tool: 'pytest'
          output-file-path: benchmark_results.json
          benchmark-data-dir-path: dev/bench
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: false
          alert-threshold: '120%'  # Alert if 20% slower
          comment-on-alert: true
          fail-on-alert: false
          alert-comment-cc-users: '@maintainers'
      
      - name: Comment PR with results
        uses: actions/github-script@v6
        if: github.event_name == 'pull_request'
        with:
          script: |
            const fs = require('fs');
            
            // Read benchmark results
            let results = {};
            try {
              results = JSON.parse(fs.readFileSync('benchmark_results.json', 'utf8'));
            } catch (e) {
              console.log('Could not read benchmark results');
              return;
            }
            
            // Format results as markdown
            let comment = '## Performance Benchmark Results\n\n';
            comment += `Platform: ${{ matrix.os }} - Python ${{ matrix.python-version }}\n\n`;
            comment += '| Test | Mean | Min | Max | StdDev |\n';
            comment += '|------|------|-----|-----|--------|\n';
            
            for (const benchmark of results.benchmarks || []) {
              const stats = benchmark.stats;
              comment += `| ${benchmark.name} | ${stats.mean.toFixed(3)}s | ${stats.min.toFixed(3)}s | ${stats.max.toFixed(3)}s | ${stats.stddev.toFixed(3)}s |\n`;
            }
            
            // Post comment
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  benchmark-regression-check:
    name: Check for Performance Regressions
    runs-on: ubuntu-latest
    needs: benchmark
    if: github.event_name == 'pull_request'
    
    steps:
      - name: Download current results
        uses: actions/download-artifact@v3
        with:
          name: benchmark-results-ubuntu-latest-py3.11
          
      - name: Check for regressions
        run: |
          # This would compare against baseline
          # For now, just ensure benchmarks completed
          if [ -f "benchmark_results.json" ]; then
            echo "Benchmarks completed successfully"
          else
            echo "Benchmarks failed to complete"
            exit 1
          fi